\documentclass[english,a4paper,11pt]{article}
\usepackage[margin=3cm]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{babel}
\usepackage{bm}
\usepackage{amsmath,amsthm}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage[final]{graphicx}
\DeclareGraphicsExtensions{.jpg,.jpeg,.pdf,.png,.mps}
\usepackage{epsfig}
\usepackage[round]{natbib}
%\setcitestyle{aysep={}} 
\setcitestyle{numbers}
\setcitestyle{square}

%\usepackage[authoryear,comma,longnamesfirst,sectionbib]{natbib} 
\usepackage{rotating}
%\setlength{\topmargin}{-1cm}
\usepackage{color}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage[bitstream-charter]{mathdesign}
\usepackage[T1]{fontenc}
\usepackage{threeparttable}
\usepackage{lipsum}
\usepackage{url} 
\usepackage{hyperref}
\usepackage{lmodern}
\date{}
\hypersetup{hidelinks}

\addtolength{\textwidth}{1em}
\addtolength{\oddsidemargin}{-1em}

\linespread{1.2}

\usepackage[font=small,skip=5pt]{caption}

%% https://ies2025.sis-statistica.it/
\title{Transition Models for Precipitation Climatology Estimation}

\author{$\mathrm{Nikolaus \ Umlauf}^\mathrm{1},  
	\  \mathrm{Reto \ Stauffer}^\mathrm{1}$\\  
	$^\mathrm{1}$\small{\emph{Universtit\"at Innsbruck}}\\
}

% for \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ",
  SweaveHooks = list(fig = function() par(mar = c(4.1, 4.1, 1, 1))))
library("transitreg")
library("qgam")
library("gamlss2")
library("gamlss.cens")
library("bamlss")
@

\begin{document}
	\maketitle


\begin{abstract}
Transition models are widely recognized for their flexibility in count data regression and are
commonly applied in ordinal regression, where they are also known as continuation ratio models.
The core concept involves modeling transition probabilities, specifically the conditional
probability of observing counts greater than a threshold. These probabilities can be estimated
using standard binary regression methods with an augmented dataset, allowing the use of any
software designed for binary response models. In this paper, we extend the application of
transition models to continuous data by employing a slicing technique that transforms continuous
observations into count-like data. This approach enables the estimation of full probabilistic
models, including distributional and quantile regression, using simple binary regression
methods. The stepwise approximation of the cumulative distribution function (CDF)
converges uniformly to the true CDF.
The proposed method is highly adaptable, seamlessly handling complex data structures,
including excess zeros and non-standard distributions. We demonstrate the robustness and
utility of this approach with an application to precipitation climatology estimation in Tyrol, 
Austria, highlighting its potential for broader applications in probabilistic modeling
\noindent 
\hspace{1cm}\\
\emph{Keywords}: transition models, distributional regression, quantile regression
\end{abstract}

\section{Introduction}\label{sec:intro}

In many applications, the response variable of interest is a count, representing non-negative
integers that often relate to a set of covariates. Traditional approaches to modeling such data,
such as the Poisson and Negative Binomial regression models, rely on fixed distributional
assumptions. While these models are widely used due to their simplicity and interpretability,
their restrictive nature can lead to mis-specifications, particularly in the presence of
overdispersion or excess zeros.

Transition models provide a flexible alternative by focusing on the conditional probabilities
of transitioning between counts. Rather than assuming a fixed distribution for the response
variable, these models allow the data to dictate the form of the distribution. By modeling
transition probabilities--representing the likelihood of observing counts greater than a
specified threshold--transition models achieve remarkable adaptability. These probabilities
can be estimated using binary regression techniques with augmented datasets, leveraging
standard software for estimation \cite{Berger:2021}.

A key strength of transition models lies in their ability to accommodate
complex data structures. They handle phenomena such as excess zeros and varying
coefficients, offering both parametric and nonparametric extensions. Additionally,
embedding transition models within the binary regression framework simplifies parameter
estimation and enhances interpretability, as coefficients directly capture the effects of
covariates on transition probabilities.

In this paper, we extend transition models to continuous response variables by employing a
slicing technique that transforms continuous observations into count-like data. This innovative
approach bridges the gap between traditional count-based transition models and advanced regression
techniques such as distributional or quantile regression. We also show that the stepwise
approximation used in this method converges uniformly to the true cumulative distribution
function (CDF), ensuring the method's theoretical soundness.

Our proposed method is highly effective in capturing complex data structures and provides a
computationally efficient way to estimate full probabilistic models using standard binary
regression techniques. We illustrate the versatility and robustness of this approach through
an application to precipitation climatology estimation in Tyrol, Austria, highlighting its potential
for use in various other domains.

\section{Transition Models} \label{sec:tm}

\subsection{Classic Count Model} \label{sec:counts}

Transition models provide a flexible framework for modeling count data by focusing on
the conditional probabilities of transitioning between counts. Unlike traditional approaches
that assume a fixed distribution for the response variable, transition models estimate
transition probabilities directly, allowing the data to determine the form of the distribution.

Following \cite{Berger:2021}, let $y_i \in \{0, 1, 2, \ldots\}$ denote the count response
variable for observation $i = 1, \ldots, n$, and let $\mathbf{x}_i = (x_{i1}, \ldots, x_{ik})^\top$
represent the covariates. The conditional transition probability, which represents the
probability of the count being larger than $r$, is defined as
\begin{equation} \label{eqn:tm}
P(y_i > r \mid y_i \geq r, \mathbf{x}_i) = F(\eta_{ir}(\boldsymbol{\alpha})), \quad r = 0, 1, \ldots,
\end{equation}
where $F(\cdot)$ is a cumulative distribution function (e.g., logistic or probit),
and $\eta_{ir}(\boldsymbol{\alpha})$ is an additive predictor given by
$$
\eta_{ir}(\boldsymbol{\alpha}) = \theta_r + \sum_{j=1}^k f_j(\mathbf{x}_i, r; \boldsymbol{\beta}),
$$
with $\boldsymbol{\alpha} = (\boldsymbol{\theta}^\top, \boldsymbol{\beta}^\top)$,
including count-specific intercepts $\boldsymbol{\theta} = (\theta_0, \theta_1, \ldots)^\top$, and $f_j(\cdot)$ as unspecified smooth
functions of the covariates and possible count-specific interactions. These functions are
estimated using regression splines.

For independent and identically distributed (i.i.d.) observations $y_i$, the probabilities
$\pi_{ir}$ for each count $r$ can be expressed recursively using the transition probabilities
\begin{eqnarray*}
\pi_{ir} = P(y_i = r \mid \mathbf{x}_i) &=& P(y_i = r \mid y_i \geq r, \mathbf{x}_i) \prod_{s=0}^{r-1} P(y_i > s \mid y_i \geq s, \mathbf{x}_i) \\
  &=& (1 - F(\eta_{ir}(\boldsymbol{\alpha}))) \prod_{s=0}^{r-1} F(\eta_{is}(\boldsymbol{\alpha})).
\end{eqnarray*}

To estimate the parameters, the underlying Markov chain $Y_{i0}, Y_{i1}, \ldots$,
with $Y_{ir} = I(y_i = r)$, is considered, where $I(\cdot)$ is the indicator function.
This allows the log-likelihood of the transition model \eqref{eqn:tm} to be written as
$$
\ell(\boldsymbol{\alpha}) = \sum_{i=1}^n \log(\pi_{ir}) =
  \sum_{i=1}^n \sum_{s=0}^{y_i} \Big[Y_{is} \log(1 - F(\eta_{ir}(\boldsymbol{\alpha}))) + (1 - Y_{is}) \log(F(\eta_{ir}(\boldsymbol{\alpha})))\Big].
$$

This formulation is equivalent to a binary model, which can be estimated using classical
software for generalized additive models (GAM, \cite{Wood17}). Therefore, the original dataset is
extended by creating new binary response variables
$(Y_{i0}, Y_{i1}, \ldots, Y_{iy_i})^\top = (0, 0, \ldots, 0, 1)$, along with a new covariate
$\boldsymbol{\Theta}_i = (0, 1, \ldots, y_i)^\top$, which is used to estimate count-specific effects
$f_j(\mathbf{x}_i, \boldsymbol{\Theta}_i)$ or simple count-specific intercepts. All other covariates values
are duplicated accordingly to match the extended structure.

Although the extended dataset for estimation can grow significantly in size, the model can
still be estimated efficiently using methods developed for GAMs tailored to handle very large
datasets, as demonstrated by \cite{Wood:2014} and \cite{Wood:2017}. Similarly, instead of
estimating GAMs, one could consider neural networks or random forests for estimation.

\subsection{Continuous Responses} \label{sec:continuous}

To extend the estimation of transition models to continuous response data $y_i \in \mathbb{R}$,
we employ a discretization approach inspired by histogram construction. Specifically, the
continuous response variable is divided into $m - 1$ intervals using predefined bin boundaries
$\zeta_1, \zeta_2, \ldots, \zeta_m$, where each interval $(\zeta_l, \zeta_{l+1}]$ is associated
with a discrete count $r$. For instance, the first interval $(\zeta_1, \zeta_2]$ corresponds
to $r = 0$, the second interval $(\zeta_2, \zeta_3]$ to $r = 1$, and so on. Each observation
$y_i$ is assigned a count $r$ based on the interval it falls into, resulting in a transformed
count response $\tilde{y}_i$.

The transformed response $\tilde{y}_i$ is then used to estimate the transition model as
described in Section~\ref{sec:counts}. This allows us to leverage the methodology developed for
count data while accommodating continuous responses.

The described discretization approach effectively provides a stepwise approximation of
the underlying smooth continuous distribution. For a continuous response variable $y_i$ with
cumulative distribution function (CDF) $F(y)$, the discretization process approximates the
probabilities of $y_i$ falling into each interval as
$$
P(\zeta_l < y_i \leq \zeta_{l+1}) = F(\zeta_{l+1}) - F(\zeta_l).
$$
These probabilities are represented by the transformed counts $\tilde{y}_i$, allowing the
transition model to reconstruct the discrete probabilities as
$$
P(\tilde{y}_i = r) = P(\zeta_r < y_i \leq \zeta_{r+1}).
$$
The transition model estimates the probability of transitioning between counts
$$
P(\tilde{y}_i > r \mid \tilde{y}_i \geq r, \mathbf{x}_i) = F(\eta_{ir}(\boldsymbol{\alpha})),
$$
and recursively computes
$$
P(\tilde{y}_i = r, \mathbf{x}_i) = P(\tilde{y}_i = r \mid \tilde{y}_i \geq r, \mathbf{x}_i) \prod_{s=0}^{r-1} P(\tilde{y}_i > s \mid \tilde{y}_i \geq s, \mathbf{x}_i).
$$
For any value $y_i \in (\zeta_l, \zeta_{l+1}]$, the CDF can be approximated by
$$
\hat{F}(y_i) = \sum_{r=0}^{l-1} P(\tilde{y}_i = r) + \frac{y_i - \zeta_l}{\zeta_{l+1} - \zeta_l} P(\tilde{y}_i = l),
$$
where the first term sums probabilities for bins below $y_i$, and the second term performs linear
interpolation within the current bin. As the number of bins $m$ increases and bin widths shrink,
this stepwise approximation converges uniformly to the true CDF $F(y)$.

Similarly, the density function can be approximated as
$$
\hat{f}(y_i) = \frac{P(\tilde{y}_i = l)}{\zeta_{l+1} - \zeta_l}, \quad y_i \in (\zeta_l, \zeta_{l+1}].
$$

After estimating the transition model, quantities such as the mean, cumulative distribution
function (CDF), quantile function, and density function can be approximated using the
reconstructed CDF and density. Predictions for the continuous response variable are obtained
by mapping the predicted counts $\tilde{y}_i$ back to the continuous scale,
typically using the midpoints of the corresponding intervals as approximations.

%\subsection{Convergence of the Stepwise Approximation} \label{sec:convergence}

%To prove that the stepwise approximation $\hat{F}(y)$ converges uniformly to the true
%cumulative distribution function (CDF) $F(y)$, we proceed as follows. The true CDF $F(y)$ is
%defined as
%$$
%F(y) = P(Y \leq y), \quad y \in \mathbb{R}.
%$$
%The stepwise approximation $\hat{F}(y)$ is given by
%$$
%\hat{F}(y) = \sum_{r=0}^{l-1} P(\tilde{y} = r) + \frac{y - \zeta_l}{\zeta_{l+1} - \zeta_l} P(\tilde{y} = l),
%$$
%where $y \in (\zeta_l, \zeta_{l+1}]$, $P(\tilde{y} = r)$ corresponds to the probability
%mass assigned to the discrete count $r$, and $\zeta_l, \zeta_{l+1}$ are the bin boundaries.
%To show uniform convergence, we need to show
%$$
%\sup_{y \in \mathbb{R}} \left| \hat{F}(y) - F(y) \right| \to 0 \quad \text{as } m \to \infty,
%$$
%where $m$ is the number of bins used in the stepwise approximation. As $m \to \infty$, the
%bin boundaries $\{\zeta_l\}_{l=1}^m$ partition $\mathbb{R}$ into intervals of shrinking width
%$$
%\max_{l} (\zeta_{l+1} - \zeta_l) \to 0 \quad \text{as } m \to \infty.
%$$

%The error between $\hat{F}(y)$ and $F(y)$ can be decomposed as
%$$
%\left| \hat{F}(y) - F(y) \right| = \underbrace{\left| \sum_{r=0}^{l-1} P(\tilde{y} = r) - F(\zeta_l) \right|}_{\text{Discrete Bin Approximation Error}} + \underbrace{\left| \frac{y - \zeta_l}{\zeta_{l+1} - \zeta_l} P(\tilde{y} = l) - \left(F(y) - F(\zeta_l)\right) \right|}_{\text{Linear Interpolation Error}}.
%$$

%The discrete bin approximation error is zero because the probability masses $P(\tilde{y} = r)$
%are defined to match the probabilities of the intervals $(\zeta_r, \zeta_{r+1}]$, such that
%$$
%P(\tilde{y} = r) = F(\zeta_{r+1}) - F(\zeta_r), \quad \sum_{r=0}^{l-1} P(\tilde{y} = r) = F(\zeta_l).
%$$

%For the linear interpolation error, within each bin $(\zeta_l, \zeta_{l+1}]$, the true CDF
%$F(y)$ can be expanded as
%$$
%F(y) = F(\zeta_l) + \frac{y - \zeta_l}{\zeta_{l+1} - \zeta_l} \big(F(\zeta_{l+1}) - F(\zeta_l)\big) + \mathcal{O}((\zeta_{l+1} - \zeta_l)^2).
%$$
%By construction,
%$$
%\frac{y - \zeta_l}{\zeta_{l+1} - \zeta_l} P(\tilde{y} = l) = \frac{y - \zeta_l}{\zeta_{l+1} - \zeta_l} \big(F(\zeta_{l+1}) - F(\zeta_l)\big).
%$$
%The linear interpolation error is thus bounded by the higher-order term
%$\mathcal{O}((\zeta_{l+1} - \zeta_l)^2)$
%$$
%\left| \frac{y - \zeta_l}{\zeta_{l+1} - \zeta_l} P(\tilde{y} = l) - \big(F(y) - F(\zeta_l)\big) \right| \leq C (\zeta_{l+1} - \zeta_l)^2,
%$$
%where $C$ is a constant depending on the second derivative of $F(y)$. Combining these results,
%the total error is bounded as
%$$
%\sup_{y \in \mathbb{R}} \left| \hat{F}(y) - F(y) \right| \leq C \max_{l} (\zeta_{l+1} - \zeta_l)^2.
%$$

%As $m \to \infty$, $\max_{l} (\zeta_{l+1} - \zeta_l) \to 0$, and hence
%$$
%\sup_{y \in \mathbb{R}} \left| \hat{F}(y) - F(y) \right| \to 0.
%$$

%Thus, the stepwise approximation $\hat{F}(y)$ converges uniformly to the true CDF $F(y)$ as
%$m \to \infty$, provided that the bin widths shrink to zero, ensuring that the
%discretization approach is a consistent method for approximating the smooth CDF $F(y)$.

\section{Software} \label{sec:software}

The proposed method is implemented in the \textsf{R} package \textbf{transitreg} \cite{transitreg},
which is available on GitHub: \texttt{https://github.com/retostauffer/transitreg}.
The development version can be installed using the following command
\begin{verbatim}
install.packages("transitreg",
  repos = c("https://gamlss-dev.R-universe.dev",
            "https://cloud.R-project.org"))
\end{verbatim}
The primary function for estimating transition models is \texttt{transireg()}.

\section{Application} \label{sec:application}

\begin{figure}[!ht]
\centering
%% \setkeys{Gin}{width=1\textwidth}
<<echo=FALSE, results=hide>>=
if(!file.exists("ts_hist.png")) {
    d <- readRDS("../ehydTirol_Tageschniederschlagssummen.rds")
    d <- subset(d, date >= (max(date) - 365*30))
    d$sqrt_pre <- sqrt(d$value)
    d$day <- as.POSIXlt(d$date)$yday

    stations <- unique(d$name)

    df <- subset(d, name == "Kirchberg in Tirol")

    set.seed(123)
    i <- sample(1:2, size = nrow(df), prob = c(0.8, 0.4), replace = TRUE)
    dtrain <- subset(df, i < 2)
    dtest <- subset(df, i > 1)

    breaks <- c(0, seq(0.2, floor(max(df$sqrt_pre)) + 1, by = 0.3))

    m <- transitreg(sqrt_pre ~ theta0 + s(theta, k = 20), data = df, breaks = breaks, censored = "left")
    #m <- transitreg(sqrt_pre ~ theta0 + s(theta, k = 20), data = df, breaks = breaks)

    nd <- data.frame("sqrt_pre" = seq(0, max(df$sqrt_pre), length.out = 101))
    py <- nd$sqrt_pre
    pm <- predict(m, newdata = nd, y = nd$sqrt_pre, type = "pdf")

    mids <- (head(breaks, -1) + tail(breaks, -1)) / 2

    b <- bamlss(sqrt_pre ~ 1, data = df, family = cnorm_bamlss)
    par <- predict(b, newdata = data.frame("sqrt_pre" = mids), type = "parameter")
    db <- family(b)$d(mids, par)

    f <- sqrt_pre ~ theta0 + s(theta, k = 20) + s(day, bs = "cc", k = 20) + te(theta, day, bs = c("cr", "cc"), k = 10)

    # ----------------------------------------------
    breaks2 <- c(0, seq(0.2, floor(max(df$sqrt_pre)) + 1, by = 0.05))
    m2 <- transitreg(f, data = dtrain, breaks = breaks2, censored = "left")
    #m2 <- transitreg(f, data = dtrain, breaks = breaks2)

    f <- sqrt_pre ~ s(day, k = 20, bs = "cc") | s(day, k = 20, bs = "cc")
    b2 <- bamlss(f, data = dtrain, family = cnorm_bamlss, binning = TRUE)

    qu <- c(0.01, 0.1, 0.5, 0.9, 0.99)

    g2 <- mqgam(sqrt_pre ~ s(day, k = 20, bs = "cc"), data = dtrain, qu = qu)

    nd <- data.frame("day" = 0:365)
    pm2 <- predict(m2, newdata = dtest, prob = qu, elementwise = FALSE)

    par <- predict(b2, newdata = dtest, type = "parameter")
    pb2 <- do.call("cbind",
      lapply(qu, function(j) {
        b2$family$q(j, par)
    }))

    pg2 <- do.call("cbind",
      lapply(qu, function(j) {
        qdo(g2, j, predict, newdata = dtest)
    }))
    pg2[pg2 < 0] <- 0 # <- lower limit

    err_b <- err_m <- err_g <- NULL
    for(j in 1:5) {
      err_b <- c(err_b, qgam::pinLoss(dtest$sqrt_pre, pb2[, j], qu[j]))
      err_m <- c(err_m, qgam::pinLoss(dtest$sqrt_pre, pm2[, j], qu[j]))
      err_g <- c(err_g, qgam::pinLoss(dtest$sqrt_pre, pg2[, j], qu[j]))
    }
    err_b <- sum(err_b)
    err_m <- sum(err_m)
    err_g <- sum(err_g)
    c(bamlss_b2 = err_b, transitreg_m2 = err_m, mqgam_g2 = err_g)

    ## Create figure
    png("ts_hist.png", units = "in", res = 200, width = 8, height = 4)
    par(mfrow = c(1, 2), mar = c(4, 4, 1, 1))

    hist(df$sqrt_pre, breaks = breaks, freq = FALSE,
      xlab = "sqrt(Precipitation)", main = NA)

    lines(pm ~ py, col = 4, lwd = 2)
    lines(db ~ mids, col = 2, lwd = 2)
    rug(df$sqrt_pre, col = rgb(0.1, 0.1, 0.1, alpha = 0.4))

    legend("center", c("TM", "CN"),
      lwd = 2, col = c(4, 2), bty = "n")

    plot(sqrt_pre ~ day, data = dtest, type = "h", col = rgb(0.1, 0.1, 0.1, alpha = 0.4),
      xlab = "Day of the year", ylab = "sqrt(Precipitation)", ylim = c(0, 11))

    j <- order(dtest$day)
    matplot(dtest$day[j], pb2[j, ], type = "l", lty = 1, col = 2, add = TRUE)
    matplot(dtest$day[j], pg2[j, ], type = "l", lty = 1, col = 3, add = TRUE)
    matplot(dtest$day[j], pm2[j, ], type = "l", lty = 1, col = 4, add = TRUE)

    err <- c("CN" = err_b, "QR" = err_g, "TM" = err_m)
    col <- c(2, 3, 4)
    i <- order(err)
    err <- err[i]
    col <- col[i]

    legend("topleft", paste(paste(names(err), "PBL ="), round(err)),
      lwd = 2, col = col, bty = "n")

    dev.off()
}
@
\includegraphics[width=0.9\textwidth]{ts_hist.png}
\caption{\label{fig:Kirchberg} Precipitation data for Kirchberg in Tirol, Austria.
  The left panel shows a histogram of the square root-transformed daily precipitation values,
  overlaid with fitted densities from a censored normal distribution (CN, red line) and
  a transition model (TM, blue line). The right panel depicts the seasonal variation in
  square root-transformed precipitation values, including empirical quantiles
  (1st, 10th, 50th, 90th, and 99th percentiles) for each day of the year.
  Quantile estimates (out-of-sample) are provided for CN, TM, and a quantile regression model (QR),
  along with their respective pinball loss (PBL) values, demonstrating the models' performance
  in capturing the distribution of precipitation.}
\end{figure}
\begin{figure}[!h]
\centering
\captionsetup{skip=-6.2cm}
<<echo=FALSE, results=hide>>=
if(!file.exists("premodel.png") & FALSE) {
  d <- readRDS("../ehydTirol_Tageschniederschlagssummen.rds")
  d <- subset(d, date >= (max(date) - 365*30))
  d$sqrt_pre <- sqrt(d$value)
  d$day <- as.POSIXlt(d$date)$yday

  stations <- unique(d$name)

  set.seed(123)
  i <- sample(stations, size = floor(length(stations) * 0.8))

  dtrain <- subset(d, name %in% i)
  dtest <- subset(d, !(name %in% i))

  breaks <- c(-0.05, seq(0.05, floor(max(d$sqrt_pre)) + 1, by = 0.1))

  f1 <- sqrt_pre ~ theta0 +
    ti(theta) +
    ti(alt) +
    ti(day,bs="cc",k=20) +
    ti(lon,lat,bs="tp",d=2,k=30) +
    ti(day,lon,lat,d=c(1,2),k=c(5,5,30),bs=c("cc","tp")) +
    ti(theta,day,lon,lat,d=c(1,1,2),k=c(5,5,30),bs=c("cr","cc","tp")) +
    ti(theta,day,bs=c("cr","cc")) +
    ti(theta,alt) +
    ti(theta,lon,lat,d=c(1,2),bs=c("cr","tp"),k=c(5,30))

  m1 <- transitreg(f1, data = dtrain, breaks = breaks)

  f2 <- sqrt_pre ~ s(day,bs="cc",k=20) + s(alt,k=5) + s(lon,lat) +
    te(day,lon,lat,d=c(1,2),bs=c("cr","tp"),k=c(5,30)) |
      s(day,bs="cc",k=8) + s(alt,k=5) + s(lon,lat) +
      te(day,lon,lat,d=c(1,2),bs=c("cr","tp"),k=c(5,30))

  m2 <- bamlss(f2, data = dtrain, family = cnorm_bamlss,
    binning = TRUE, light = TRUE)

  qu <- c(0.01, 0.1, 0.5, 0.9, 0.99)
  err1 <- err2 <- NULL
  par <- predict(m2, newdata = dtest, type = "parameter")
  for(j in qu) {
    pj1 <- predict(m1, newdata = dtest, prob = j)
    pj2 <- family(m2)$q(j, par)
    err1 <- c(err1, sum(qgam::pinLoss(dtest$sqrt_pre, pj1, j)))
    err2 <- c(err2, sum(qgam::pinLoss(dtest$sqrt_pre, pj2, j)))
  }

#R> err1
#[1]   2314.927  23149.270 112589.150  71447.505  11941.508
#R> err2
#[1]   2314.927  23149.270 112545.307  71472.810  12087.671
#R> round((err1 - err2) / err2 * 100, 2)
#[1]  0.00  0.00  0.04 -0.04 -1.21

  download.file("http://bamlss.org/misc/precipitation_clim.tar.gz", "clim.tar.gz")
  untar("clim.tar.gz", exdir = ".")

  download.file("https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_AUT_1.json.zip", "AT-gadm.zip")
  unzip("AT-gadm.zip")

  library("raster")
  library("stars")
  library("sf")

  dem <- st_as_stars(readRDS("dem.rds"))
  dem <- st_transform(dem, crs = 4326)

  Tyrol <- "gadm41_AUT_1.json" |>
    read_sf() |>
    subset(NAME_1 == "Tirol") |>
    st_geometry()

  dem <- mask(readRDS("dem.rds"), as(Tyrol, "Spatial"))
  dem <- crop(dem, as(Tyrol, "Spatial"))

  writeRaster(dem, "dem.tif", format = "GTiff", overwrite = TRUE)

  nd <- as.data.frame(coordinates(dem))
  names(nd) <- c("lon", "lat")
  nd$alt <- values(dem)

  days <- c(15, 46, 74, 105, 135, 166, 196, 227, 258, 288, 319, 349)
  names(days) <- month.name

  for(j in names(days)) {
    print(j)

    nd$day <- days[j]
    nd[[paste0("m1_", j)]] <- predict(m1, newdata = nd, prob = 0.99)

    par <- predict(m2, newdata = nd, type = "parameter")
    par <- as.data.frame(par)
    p2 <- family(m2)$q(0.99, par)
    nd[[paste0("m2_", j)]][!is.na(nd$alt)] <- p2
  }

  r_list <- lapply(names(days), function(j) {
    a <- nd[, c("lon", "lat", paste0("m1_", j))]
    a[[paste0("m1_", j)]] <- a[[paste0("m1_", j)]]^2
    rasterFromXYZ(a)
  })
  pred <- stack(r_list)
  names(pred) <- gsub("m1_", "", names(pred))
  crs(pred) <- crs(dem)

  library("ggplot2")
  library("dplyr")
  library("tidyr")

  pred_df <- as.data.frame(pred, xy = TRUE)
  pred_df <- pred_df[, c("x", "y", names(pred))]
  
  pred_long <- pivot_longer(
    pred_df, 
    cols = names(pred), 
    names_to = "Season", 
    values_to = "value"
  )

  pred_long$Season <- factor(pred_long$Season, levels = names(pred))

  rain_colors <- function (n, h = c(-160, -38), c. = c(10, 80), l = c(86, 39), 
    power = c(2.75806451612903, 1), fixup = FALSE, gamma = NULL, 
    alpha = 1, ...) 
  {
    if (!is.null(gamma)) 
        warning("'gamma' is deprecated and has no effect")
    if (n < 1L) 
        return(character(0L))
    h <- rep(h, length.out = 2L)
    c <- rep(c., length.out = 2L)
    l <- rep(l, length.out = 2L)
    power <- rep(power, length.out = 2L)
    rval <- seq(1, 0, length = n)
    rval <- hex(polarLUV(L = l[2L] - diff(l) * rval^power[2L], 
        C = c[2L] - diff(c) * rval^power[1L], H = h[2L] - diff(h) * 
            rval), fixup = fixup, ...)
    if (!missing(alpha)) {
        alpha <- pmax(pmin(alpha, 1), 0)
        alpha <- format(as.hexmode(round(alpha * 255 + 1e-04)), 
            width = 2L, upper.case = TRUE)
        rval <- paste(rval, alpha, sep = "")
    }
    return(rval)
  }

  rc <- rain_colors(100)

  png("predictions.png", units = "in", res = 200, width = 10, height = 16)

  xr <- range(pred_long$x)
  xb <- round(seq(xr[1] + 0.1*abs(diff(xr)), xr[2] - 0.1*abs(diff(xr)), length = 5), 2)
  yr <- range(pred_long$y)
  yb <- round(seq(yr[1] + 0.1*abs(diff(yr)), yr[2] - 0.1*abs(diff(yr)), length = 5), 2)

  ggplot() +
    geom_tile(data = pred_long, aes(x = x, y = y, fill = value)) +
    geom_sf(data = Tyrol, fill = NA, color = "black", size = 0.5) + # Overlay Tyrol border
    facet_wrap(~ Season, nrow = 4, ncol = 3) + # Correct ordering
    scale_fill_gradientn(colors = rc, na.value = "transparent") + # Custom color scale
    scale_x_continuous(breaks = xb) + # Adjust x-axis ticks
    scale_y_continuous(breaks = yb) + # Adjust y-axis ticks
    coord_sf(crs = st_crs(Tyrol)) + # Respect CRS and aspect ratio
    theme_minimal() +
    labs(title = NULL,
         x = "Longitude",
         y = "Latitude",
         fill = "Precipitation\n[mm]")

   dev.off()
}
@
\includegraphics[width=1\textwidth]{predictions.png}
\caption{\label{fig:clim} Estimated climatology for the 99\% quantile precipitation using
  the TM model for selected days representing the 12 months of the year. The figure
  highlights spatial and seasonal variations in precipitation across Tyrol. Summer
  months exhibit the highest precipitation levels, whereas in winter, the southern
  regions experience the most precipitation.}
\end{figure}

In this application, we analyze 30 years of precipitation data (1992-2021)
from the Tyrolean Alps to estimate a 
climatology for precipitation. Figure~\ref{fig:Kirchberg} visualizes the raw data for the station 
Kirchberg in Tirol. The left panel presents a histogram of the square root-transformed precipitation 
values, overlaid with density estimates from a transition model (TM) and a parametric censored normal (CN) model. Notably, the TM captures the inherent structure of the data more effectively, 
particularly the spike at zero precipitation. This demonstrates the flexibility of the TM approach, 
allowing it to directly model key features such as excess zeros.
The right panel displays out-of-sample precipitation data along with predicted quantiles. The 
corresponding pinball loss (PBL) values for the TM, CN, and quantile regression (QR) models are 
presented in the legend. Among these, the TM model provides the best prediction of the
out-of-sample distribution, as indicated by the lowest PBL. Notably, the 99\% quantile
predicted by the CN model is slightly higher than those of the TM and QR models, while
the 50\% quantile estimate from the QR model exceeds those of the TM and CN models.

Finally, to estimate a full climatology and validate the TM compared to the CN model, we split 
the data into training and testing sets by excluding 21 (20\%) of the total 105 meteorological 
stations from the training data. We computed the out-of-sample pinball loss (PBL) for the 0.01, 
0.1, 0.5, 0.9, and 0.99 quantiles. The TM achieved the lowest overall PBL of 221,442, compared 
to 221,570 for the CN. For the 99\% quantile, the TM's PBL was 1.21\% lower than the CN, 
demonstrating slightly better performance.

Although the transformed dataset used for the TM contained over 10 million observations, the 
estimation time was approximately 15 minutes, significantly faster than the 60 minutes required 
for the CN. The estimated climatology for the 99\% quantile using the TM is illustrated in 
Figure~\ref{fig:clim}, showing selected days of the year representing the 12 months. The figure 
highlights the TM's ability to capture space-time varying seasonal effects. Notably, the
highest precipitation amounts occur in the summer months, while in winter, the highest 
precipitation is concentrated in the southern parts of Tyrol.

\section{Summary} \label{sec:summary}

In this paper, we present an innovative application of transition models (TM) for estimating 
precipitation climatology, focusing on their flexibility and adaptability in handling complex 
data structures. Transition models, traditionally used for count data, are extended to 
continuous data using a slicing technique that transforms continuous observations into
count-like representations. This approach enables the estimation of full probabilistic models, 
including distributional and quantile regression, using standard binary regression techniques.

Through an application to 30 years of precipitation data from Tyrol, Austria, we demonstrate
the robustness of the TM approach. The results show that the TM outperforms the censored normal 
(CN) model, achieving a lower pinball loss (PBL) across all quantiles.
For the critical 99\% quantile, the TM achieves a 1.21\% lower PBL compared to the CN,
while maintaining computational efficiency. The estimated climatology highlights seasonal 
precipitation variations, with summer months showing the highest precipitation levels
and winter precipitation concentrated in the southern regions of Tyrol.
This study underscores the potential of transition models for broader applications in 
probabilistic modeling, offering a computationally efficient and theoretically sound framework 
for analyzing complex data.

%\bibliographystyle{plainnat}
%\bibliographystyle{apalike}
\bibliographystyle{unsrt}
\bibliography{ies2025.bib}

%\listofchanges

\end{document}

